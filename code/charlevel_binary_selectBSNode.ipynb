{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hijS08WitOY",
        "outputId": "7f04078e-2da9-4098-cfc2-11e5d30d4167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "700\n",
            "705\n",
            "710\n",
            "715\n",
            "720\n",
            "725\n",
            "gs average: 130.32\n",
            "725\n",
            "730\n",
            "735\n",
            "740\n",
            "745\n",
            "750\n",
            "gs average: 116.04545454545455\n",
            "750\n",
            "755\n",
            "760\n",
            "765\n",
            "770\n",
            "gs average: 112.29411764705883\n",
            "775\n",
            "780\n",
            "785\n",
            "790\n",
            "795\n",
            "gs average: 129.56521739130434\n",
            "800\n",
            "805\n",
            "810\n",
            "815\n",
            "820\n",
            "gs average: 118.17391304347827\n",
            "825\n",
            "830\n",
            "835\n",
            "840\n",
            "845\n",
            "gs average: 116.25\n",
            "850\n",
            "855\n",
            "860\n",
            "865\n",
            "870\n",
            "gs average: 119.22222222222223\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install --no-cache-dir transformers datasets\n",
        "#!pip install --no-cache-dir datasets\n",
        "import sys\n",
        "import csv\n",
        "from transformers import pipeline\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "class BSNode:\n",
        "\n",
        "    def __init__(self, data, prob):\n",
        "        self.data = data\n",
        "        self.prob = prob\n",
        "        self.exclusion = []\n",
        "        self.explored = False\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "\n",
        "    def isLeaf(self):\n",
        "        if(self.left):\n",
        "            return False\n",
        "        if(self.right):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "\n",
        "    #prints out node, left, data/prob/explored, then right\n",
        "    def printNode(self):\n",
        "        if(self.isLeaf()):\n",
        "            print(self.data, self.prob, self.explored)\n",
        "        else:\n",
        "            if(self.left):\n",
        "                self.left.printNode()\n",
        "\n",
        "            print(self.data, self.prob, self.explored)\n",
        "\n",
        "            if(self.right):\n",
        "                self.right.printNode()\n",
        "\n",
        "\n",
        "\n",
        "    #returns the node with the lowest probability\n",
        "    # which hasn't yet been explored, if every node has been explored\n",
        "    # returns None\n",
        "    def lowestProb(self):\n",
        "        if(self.isLeaf()): # cannot explore further\n",
        "            if(self.explored): # already explored, exit\n",
        "                return None\n",
        "            else:\n",
        "                return self # only one node here, so must be the lowest\n",
        "\n",
        "        else: #explore right and left\n",
        "            left_low = right_low = None\n",
        "\n",
        "            if(self.left):\n",
        "                left_low = self.left.lowestProb()\n",
        "            if(self.right):\n",
        "                right_low = self.right.lowestProb()\n",
        "\n",
        "            if(not left_low and not right_low): #both branches explored, which means this node already is fully explored\n",
        "                self.explored = True\n",
        "                return None\n",
        "\n",
        "            if(not left_low):\n",
        "                return right_low\n",
        "            if(not right_low):\n",
        "                return left_low\n",
        "\n",
        "            # finally if here, both left and right have not been explored\n",
        "            # so chooose the lower probability\n",
        "            if(left_low.prob < right_low.prob):\n",
        "                return left_low\n",
        "            else:\n",
        "                return right_low\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def binary_select_iterative_BSNode(classifier, query, attack_class = 0, BSStruct = None, initial_prob = None): #for analysis\n",
        "\n",
        "    query_count = 0\n",
        "\n",
        "    # set up initial root BSNode\n",
        "    if(not BSStruct):\n",
        "        if(not initial_prob):\n",
        "            # get initial probability for query\n",
        "            initial_prob = classifier(query)[0][attack_class][\"score\"]\n",
        "\n",
        "        query_count += 1\n",
        "\n",
        "        BSStruct = BSNode(query, initial_prob)\n",
        "\n",
        "\n",
        "\n",
        "    initial_prob = BSStruct.prob\n",
        "\n",
        "    # search to find lowest prob node, which has also not been explored\n",
        "    cur_struct = BSStruct.lowestProb()\n",
        "    final_prob = initial_prob\n",
        "\n",
        "    #split_query = query.split()\n",
        "    split_query = [x for x in query] #char level split\n",
        "\n",
        "    if(len(cur_struct.exclusion) == 1): # only 1 word, no need to explore more, just return\n",
        "        cur_struct.explored = True # exploring!\n",
        "        return cur_struct.exclusion[0], cur_struct.prob, 0, BSStruct #exclusion list with only 1 item is that word's position in the text\n",
        "    else: # set up start and end\n",
        "        if(len(cur_struct.exclusion) == 0): # root node, no exclusion, entire text\n",
        "            start = 0\n",
        "            end = len(split_query) - 1#makes slicing easier\n",
        "        else:\n",
        "            start = cur_struct.exclusion[0]\n",
        "            end = start + len(cur_struct.exclusion) - 1\n",
        "\n",
        "\n",
        "    while start < end:\n",
        "        mid = start + (end - start) // 2\n",
        "\n",
        "        first_exclusion = list(range(start, mid+1))\n",
        "        second_exclusion = list(range(mid+1, end+1))\n",
        "\n",
        "        first_list = []\n",
        "        second_list =[]\n",
        "\n",
        "        for i in range(len(split_query)):\n",
        "          if(i not in first_exclusion):\n",
        "            first_list.append(split_query[i])\n",
        "        #first_part = ' '.join(first_list)\n",
        "        first_part = ''.join(first_list) #character level rejoin\n",
        "\n",
        "        for i in range(len(split_query)):\n",
        "          if(i not in second_exclusion):\n",
        "            second_list.append(split_query[i])\n",
        "        #second_part = ' '.join(second_list)\n",
        "        second_part = ''.join(second_list) #char level rejoin\n",
        "\n",
        "        first_prob = classifier(first_part)[0][attack_class][\"score\"]\n",
        "        second_prob = classifier(second_part)[0][attack_class][\"score\"]\n",
        "        query_count += 2  # Increment for both sentiment analysis requests\n",
        "\n",
        "        # add information to Binary structure as we get it\n",
        "        cur_struct.left = BSNode(' '.join([split_query[i] for i in first_exclusion]), first_prob)\n",
        "        cur_struct.right = BSNode(' '.join([split_query[i] for i in second_exclusion]), second_prob)\n",
        "        cur_struct.left.exclusion = first_exclusion\n",
        "        cur_struct.right.exclusion = second_exclusion\n",
        "\n",
        "\n",
        "\n",
        "        first_drop = initial_prob - first_prob\n",
        "        second_drop = initial_prob - second_prob\n",
        "\n",
        "        if first_drop > second_drop:\n",
        "            if(len(first_exclusion) == 1):\n",
        "                # need to note that we are exploring this node so we don't again!!!!\n",
        "                cur_struct.left.explored = True\n",
        "\n",
        "            final_prob = first_prob\n",
        "\n",
        "            cur_struct = cur_struct.left\n",
        "            start = first_exclusion[0]\n",
        "            end = first_exclusion[-1]\n",
        "        else:\n",
        "            if(len(second_exclusion) == 1):\n",
        "                cur_struct.right.explored = True\n",
        "\n",
        "            final_prob = second_prob\n",
        "\n",
        "            cur_struct = cur_struct.right\n",
        "            start = second_exclusion[0]\n",
        "            end = second_exclusion[-1]\n",
        "\n",
        "\n",
        "        most_influential_pos = start\n",
        "\n",
        "\n",
        "    return most_influential_pos, final_prob, query_count, BSStruct\n",
        "\n",
        "\n",
        "\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "\n",
        "    return list(synonyms - {word})\n",
        "\n",
        "# takes in an output from textclassifier and returns the highest probability\n",
        "# needed for multiclass problems\n",
        "def pred_class(preds):\n",
        "    pred = 0\n",
        "    score = preds[0]['score']\n",
        "\n",
        "    for i in range(1, len(preds)):\n",
        "        cur_score = preds[i]['score']\n",
        "        if(cur_score > score):\n",
        "            pred = i\n",
        "            score = cur_score\n",
        "\n",
        "    return pred\n",
        "\n",
        "def char_replace(classifier, query, initial_prob, replace_pos, attack_class):\n",
        "    split_query = [x for x in query]\n",
        "    replace_char = split_query[replace_pos]\n",
        "    charfile = open('/content/drive/MyDrive/Colab Notebooks/selected.neighbors')\n",
        "    char_dict = {}\n",
        "    charfile.readline()\n",
        "\n",
        "    for line in charfile:\n",
        "      cur_line = line.split('\\t')\n",
        "      x = cur_line[0][0]\n",
        "      x_repl = cur_line[1][1]\n",
        "      char_dict[x] = x_repl\n",
        "\n",
        "    if(replace_char in char_dict):\n",
        "      syn_char = char_dict[replace_char]\n",
        "    else: # choose random character\n",
        "      syn_char = random.choice(list(char_dict.values()))\n",
        "\n",
        "    split_query[replace_pos] = syn_char\n",
        "    cur_query = ''.join(split_query)\n",
        "    query_count = 1\n",
        "    cur_preds = classifier(cur_query)[0]\n",
        "    cur_prob = cur_preds[attack_class]['score']\n",
        "    cur_label = pred_class(cur_preds)\n",
        "\n",
        "    # this means the attack flipped the label\n",
        "    if(cur_label != attack_class):\n",
        "      return True, cur_query, cur_prob, query_count\n",
        "    else:# did not flip label\n",
        "      return False, cur_query, cur_prob, query_count\n",
        "\n",
        "\n",
        "# takes in a text and attempts to flip the label with binary selection and character synonym replacement\n",
        "def BS_CR(classifier, query, attack_class, k = -1):\n",
        "    done = False\n",
        "    query_count = 0\n",
        "    cur_struct = None\n",
        "    initial_prob = classifier(query)[0][attack_class][\"score\"]\n",
        "    query_count += 1\n",
        "\n",
        "    cur_query = query\n",
        "    chars_changed = 0\n",
        "    while(not done):\n",
        "        # call binary select\n",
        "        #\n",
        "        if(cur_struct):\n",
        "            replace_pos, final_prob, queries, cur_struct = binary_select_iterative_BSNode(classifier, query, attack_class, BSStruct = cur_struct, initial_prob = initial_prob)\n",
        "        else:\n",
        "            replace_pos, final_prob, queries, cur_struct = binary_select_iterative_BSNode(classifier, query, attack_class, initial_prob = initial_prob)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #cur_struct.printNode()\n",
        "\n",
        "        #add new amount of queries\n",
        "        query_count += queries\n",
        "\n",
        "\n",
        "        # try and replace with wordnet replace\n",
        "        success, cur_query, cur_prob, queries = char_replace(classifier, cur_query, initial_prob, replace_pos, attack_class)\n",
        "        chars_changed += 1\n",
        "\n",
        "        query_count += queries\n",
        "        #print(success, cur_query, cur_prob, query_count)\n",
        "\n",
        "        if(success):\n",
        "            #cur_struct.printNode()\n",
        "            return True, cur_query, query_count, cur_prob\n",
        "        else:\n",
        "            #check if everything explored\n",
        "            if(not cur_struct.lowestProb()):\n",
        "                done = True\n",
        "\n",
        "        # check if k hit\n",
        "        if(k != -1 and chars_changed >= k):\n",
        "            done = True\n",
        "\n",
        "    return False, cur_query, query_count, cur_prob\n",
        "\n",
        "\n",
        "def GreedySelect(classifier, query, attack_class = 0, initial_prob = None):\n",
        "\n",
        "    if(not initial_prob):\n",
        "        initial_prob = classifier(query)[0][attack_class][\"score\"]\n",
        "\n",
        "    prob_drops = {}\n",
        "\n",
        "    #split_query = query.split()\n",
        "    split_query = [x for x in query]\n",
        "\n",
        "    # remove each char and then note the drop in probability\n",
        "    for i in range(len(split_query)):\n",
        "        cur_query = ''.join(split_query[:i] + split_query[i+1:])\n",
        "        cur_prob = classifier(cur_query)[0][attack_class][\"score\"]\n",
        "        prob_drops[i] = initial_prob - cur_prob\n",
        "\n",
        "\n",
        "    return prob_drops\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def GS_CR(classifier, query, attack_class, k = -1):\n",
        "    done = False\n",
        "    query_count = 0\n",
        "    cur_struct = None\n",
        "    initial_prob = classifier(query)[0][attack_class][\"score\"]\n",
        "\n",
        "    query_count += 1\n",
        "    chars_changed = 0\n",
        "    cur_query = query\n",
        "\n",
        "    prob_drops = GreedySelect(classifier, query, attack_class, initial_prob)\n",
        "    query_count += len(prob_drops)\n",
        "\n",
        "    prob_sorted = dict(sorted(prob_drops.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    while(not done):\n",
        "\n",
        "        replace_pos = list(prob_sorted.keys())[0]\n",
        "        prob_sorted.pop(replace_pos)\n",
        "\n",
        "        # try and replace with wordnet replace\n",
        "        success, cur_query, cur_prob, queries = char_replace(classifier, cur_query, initial_prob, replace_pos, attack_class)\n",
        "        chars_changed += 1\n",
        "\n",
        "        query_count += queries\n",
        "        #print(success, cur_query, cur_prob, query_count)\n",
        "\n",
        "        if(success):\n",
        "            return True, cur_query, query_count, cur_prob\n",
        "        else:\n",
        "            #check if everything explored\n",
        "            if(len(prob_sorted) == 0):\n",
        "                done = True\n",
        "\n",
        "        if(k != -1 and chars_changed >= k):\n",
        "            done = True\n",
        "\n",
        "    return False, cur_query, query_count, cur_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def testsst2(start = 0):\n",
        "    classifier =  pipeline(\"text-classification\", model=\"celine98/canine-s-finetuned-sst2\", max_length=512, truncation = True, return_all_scores = True)\n",
        "    dataset = load_dataset('glue', 'sst2')\n",
        "    test_data = dataset['validation']\n",
        "    end = start + 24\n",
        "    #binaryout = csv.writer(open(\"/content/drive/MyDrive/Colab Notebooks/sst2_test_BS-CR_\" + str(start)+'-'+str(end)+\"_k_50.tsv\", 'w'), delimiter = '\\t')\n",
        "    greedyout = csv.writer(open(\"/content/drive/MyDrive/Colab Notebooks/sst2_test_GS-CR_\" + str(start)+'-'+str(end)+\"_k_50.tsv\", 'w'), delimiter = '\\t')\n",
        "\n",
        "    bin_queries = 0\n",
        "    greedy_queries = 0\n",
        "    total = 0\n",
        "    cur_id = 0\n",
        "    for cur in test_data:\n",
        "        if(cur_id < start):\n",
        "          cur_id +=1\n",
        "          continue\n",
        "\n",
        "        if(cur_id % 5 == 0):\n",
        "          print(cur_id)\n",
        "        #make sure the classifier is not already failing\n",
        "        cur_pred = classifier(cur['sentence'])[0]\n",
        "        predicted = pred_class(cur_pred)\n",
        "        if(predicted != int(cur['label'])):\n",
        "            success = 'Skipped'\n",
        "            #binaryout.writerow([cur_id, success, cur['sentence'], 0, 0.0])\n",
        "            greedyout.writerow([cur_id, success, cur['sentence'], 0, 0.0])\n",
        "            cur_id += 1\n",
        "            continue\n",
        "\n",
        "        #success, final_query, query_count, final_prob = BS_CR(classifier, cur['sentence'], int(cur['label']), 50)\n",
        "        #binaryout.writerow([cur_id, success, final_query, query_count, final_prob])\n",
        "        #bin_queries += query_count\n",
        "\n",
        "        success, final_query, query_count, final_prob = GS_CR(classifier, cur['sentence'], int(cur['label']), 50)\n",
        "        greedyout.writerow([cur_id, success, final_query, query_count, final_prob])\n",
        "        greedy_queries += query_count\n",
        "        total += 1\n",
        "\n",
        "        cur_id += 1\n",
        "        if(cur_id > end):\n",
        "            break\n",
        "    #print('bs average:', bin_queries/total)\n",
        "    print(\"gs average:\", greedy_queries/total)\n",
        "\n",
        "\n",
        "for i in range(700, 875, 25):\n",
        "  testsst2(i)\n",
        "\n",
        "#classifier =  pipeline(\"text-classification\", model=\"textattack/distilbert-base-uncased-imdb\", max_length=512, truncation = True, return_all_scores = True)\n",
        "\n",
        "#query_count = 0\n",
        "#classifier =  pipeline(\"text-classification\", model=\"celine98/canine-s-finetuned-sst2\", max_length=512, truncation = True, return_all_scores = True)\n",
        "#print(GS_CR(classifier, \"that loves its characters and communicates something rather beautiful about human nature\", 1))\n",
        "#print(BS_CR(classifier, \"that loves its characters and communicates something rather beautiful about human nature\", 1))\n",
        "\n",
        "#text = 'the worst movie ever'\n",
        "#success, final_query, query_count, final_prob = BS_WNR(classifier, text, 0)\n",
        "#print(success, final_query, query_count, final_prob)\n",
        "\n",
        "#text = 'the worst movie ever . . . not good once'\n",
        "#success, final_query, query_count, final_prob = BS_WNR(classifier, text, 0)\n",
        "#print(success, final_query, query_count, final_prob)\n",
        "\n",
        "#text = 'the worst movie ever . . . not good once'\n",
        "#success, final_query, query_count, final_prob = GS_WNR(classifier, text, 0)\n",
        "#print(success, final_query, query_count, final_prob)\n",
        "\n",
        "#text = '''\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.'''\n",
        "#success, final_query, query_count, final_prob = BS_WNR(classifier, text, 0)\n",
        "#print(success, final_query, query_count, final_prob)\n",
        "\n",
        "#text = '''\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.'''\n",
        "#success, final_query, query_count, final_prob = GS_WNR(classifier, text, 0)\n",
        "#print(success, final_query, query_count, final_prob)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVpPbLhn4DhA",
        "outputId": "3e89ae01-9ebf-4280-84b6-0b9d59bff846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'a': 'â', 'b': 'ḃ', 'c': 'ĉ', 'd': 'ḑ', 'e': 'ê', 'f': 'ḟ', 'g': 'ǵ', 'h': 'ĥ', 'i': 'î', 'j': 'ĵ', 'k': 'ǩ', 'l': 'ᶅ', 'm': 'ḿ', 'n': 'ň', 'o': 'ô', 'p': 'ṕ', 'q': 'ʠ', 'r': 'ř', 's': 'ŝ', 't': 'ẗ', 'u': 'ǔ', 'v': 'ṽ', 'w': 'ẘ', 'x': 'ẍ', 'y': 'ŷ', 'z': 'ẑ', 'A': 'Â', 'B': 'Ḃ', 'C': 'Ĉ', 'D': 'Ď', 'E': 'Ê', 'F': 'Ḟ', 'G': 'Ĝ', 'H': 'Ĥ', 'I': 'Î', 'J': 'Ĵ', 'K': 'Ǩ', 'L': 'Ĺ', 'M': 'Ḿ', 'N': 'Ň', 'O': 'Ô', 'P': 'Ṕ', 'Q': 'Q', 'R': 'Ř', 'S': 'Ŝ', 'T': 'Ť', 'U': 'Û', 'V': 'Ṽ', 'W': 'Ŵ', 'X': 'Ẍ', 'Y': 'Ŷ', 'Z': 'Ẑ'}\n",
            "Ĉ\n"
          ]
        }
      ],
      "source": [
        "charfile = open('/content/drive/MyDrive/Colab Notebooks/selected.neighbors')\n",
        "char_dict = {}\n",
        "charfile.readline()\n",
        "\n",
        "for line in charfile:\n",
        "  cur_line = line.split('\\t')\n",
        "  x = cur_line[0][0]\n",
        "  x_repl = cur_line[1][1]\n",
        "  char_dict[x] = x_repl\n",
        "\n",
        "print(char_dict)\n",
        "\n",
        "import random\n",
        "print(random.choice(list(char_dict.values())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Snm0_Fa6lBUK",
        "outputId": "a21a806a-56c7-4a84-a518-b1839ca5c8db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[{'label': 'LABEL_0', 'score': 0.8160994052886963}, {'label': 'LABEL_1', 'score': 0.1839006543159485}]]\n",
            "[[{'label': 'LABEL_0', 'score': 0.9941608309745789}, {'label': 'LABEL_1', 'score': 0.005839199293404818}]]\n"
          ]
        }
      ],
      "source": [
        "classifier =  pipeline(\"sentiment-analysis\", model=\"textattack/distilbert-base-uncased-imdb\", max_length=512, truncation = True, return_all_scores = True)\n",
        "print(classifier(\"the whip moving-picture_show e'er . . . non adept at_one_time\"))\n",
        "char_classifier =  pipeline(\"text-classification\", model=\"celine98/canine-s-finetuned-sst2\", max_length=512, truncation = True, return_all_scores = True)\n",
        "print(char_classifier(\"contains no wit , only labored gags\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}